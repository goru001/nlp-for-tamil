{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Tamil Wiki Dump and Extract all articles to local disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0AWt_a15m68f"
   },
   "source": [
    "This is first of the series of notebooks for language model creation.\n",
    "\n",
    "We parse the daily wikidump instead of crawling the website of wikipedia. \n",
    "If you only need a small subset of article, you can choose to use the crawler available at\n",
    "https://github.com/vanangamudi/newspaper-crawler-scripts/blob/master/tamil/crawler-tawiki.py (Vanangamudi)\n",
    "\n",
    "As of March 31, 2019, the Tamil Wikipedia has more than 120,000 articles - to be precise: 127381 articles.\n",
    "This notebook uses wikiextractor module to download the xml dump and extract these articles.\n",
    "It then saves only the plain text extracts, in 447 files under directories named AA, AB, AC, AD, AE,\n",
    "each folder containing 100 files.\n",
    "\n",
    "Each file itself is roughly 1MB large and holds upto 200 or 300 articles - each article having a doc id and url information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZ9dGP92ej_l"
   },
   "source": [
    "## Step 1. Download Tamil Wiki Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "colab_type": "code",
    "id": "ZSBr7MTbYvjy",
    "outputId": "3fc42b16-f3ea-4366-fe7b-bb4ceed5f9e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL transformed to HTTPS due to an HSTS policy\n",
      "--2019-04-03 16:10:21--  https://dumps.wikimedia.org/tawiki/latest/tawiki-latest-pages-articles.xml.bz2\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.155.106, 2620:0:861:4:208:80:155:106\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.155.106|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 142062878 (135M) [application/octet-stream]\n",
      "Saving to: ‘tawiki-latest-pages-articles.xml.bz2’\n",
      "\n",
      "tawiki-latest-pages 100%[===================>] 135.48M  1.97MB/s    in 69s     \n",
      "\n",
      "2019-04-03 16:11:30 (1.96 MB/s) - ‘tawiki-latest-pages-articles.xml.bz2’ saved [142062878/142062878]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://dumps.wikimedia.org/tawiki/latest/tawiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Unzip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nTNR3ZbqYvuf"
   },
   "outputs": [],
   "source": [
    "!bunzip2 tawiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "uE3b_XLQYvy0",
    "outputId": "92e45e39-703a-4779-acc6-1ec18afcd28f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1168352\r\n",
      "-rw-r--r-- 1 ravi_annaswamy ravi_annaswamy 1196373931 Mar 21 08:46 tawiki-latest-pages-articles.xml\r\n",
      "-rw-r--r-- 1 ravi_annaswamy ravi_annaswamy      12279 Apr  3 16:12 Tamil Wikipedia Downloader and Article Extractor.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Get wikiextract software from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "QC3gOA8BYvr6",
    "outputId": "4ec9cad0-16c2-44e5-f525-71cbc04462cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'wikiextractor'...\n",
      "remote: Enumerating objects: 523, done.\u001b[K\n",
      "remote: Total 523 (delta 0), reused 0 (delta 0), pack-reused 523\u001b[K\n",
      "Receiving objects: 100% (523/523), 454.22 KiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (297/297), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/attardi/wikiextractor.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "-Xxl8QviaFME",
    "outputId": "f56022f5-2c37-4067-9b5d-1eb6ce724608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamil Wikipedia Downloader and Article Extractor.ipynb\twikiextractor\r\n",
      "tawiki-latest-pages-articles.xml\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Run extractor on the xml dump quietly and ignoring embedded tables and other templates in wiki articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LevWTYhTaH5x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./wikiextractor/WikiExtractor.py:2432: DeprecationWarning: Flags not at the start of the expression '\\\\[(((?i)bitcoin:|ftp' (truncated)\n",
      "  re.S | re.U)\n",
      "./wikiextractor/WikiExtractor.py:2439: DeprecationWarning: Flags not at the start of the expression '^(http://|https://)(' (truncated)\n",
      "  re.X | re.S | re.U)\n"
     ]
    }
   ],
   "source": [
    "!python ./wikiextractor/WikiExtractor.py tawiki-latest-pages-articles.xml --no-templates -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "53imvwuPaH3S",
    "outputId": "dce89a71-f781-4414-d88b-c4fb83b02ae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamil Wikipedia Downloader and Article Extractor.ipynb\ttext\r\n",
      "tawiki-latest-pages-articles.xml\t\t\twikiextractor\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Get a list of all article collection files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gNNGlq5uaH0I"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "flist=glob.glob('text/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lwuqbrs7aHyV",
    "outputId": "861a3ebb-e22b-4a5c-a724-9749ffdbc3c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text/AE/wiki_12', 'text/AE/wiki_17']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flist[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have one issue. Wikiextractor has named the files as wiki_00..wiki_99 is each of the 5 folders. We would like to rename the files with prefix of the directory to make them all unique.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DfY7y5G7aHv4",
    "outputId": "94d05a2c-8695-49cb-f58e-cb0440e7216e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text/AE/AE_wiki_12.txt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def future_name(fn):\n",
    "  a,b,c=fn.split('/')\n",
    "  return '/'.join([a,b,b+'_'+c+'.txt'])\n",
    "future_name(flist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DsF5w3zJekAk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for f in flist:\n",
    "  os.rename(f,future_name(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "V2-ohW2kekAq",
    "outputId": "d8108ade-7b2f-4d2d-e2ce-12868d27e660"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447, ['text/AE/AE_wiki_46.txt', 'text/AE/AE_wiki_34.txt'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flist2=glob.glob('text/*/*')\n",
    "len(flist2),flist2[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. View first few sentences of text of the first file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doc id=\"438264\" url=\"https://ta.wikipedia.org/wiki?curid=438264\" title=\"இந்திய திரைப்படம் நடிகைகள் பட்டியல்\">\n",
      "இந்திய திரைப்படம் நடிகைகள் பட்டியல்\n",
      "\n",
      "அ\n",
      "\n",
      "அபிராமி\n",
      "\n",
      "அதாசர்மா\n",
      "\n",
      "அதிதிசர்மா\n",
      "\n",
      "அதிதிகோவிட்ரிக்கர்\n",
      "\n",
      "அதிதிராவ் ஹைடரி\n",
      "\n",
      "அதீதிகுப்தா\n",
      "\n",
      "அஹானாகிருஷ்ணா\n",
      "\n",
      "அந்த்ரிடாரே\n",
      "\n",
      "அகன்ஸாஜூநிஜா\n",
      "\n",
      "அக்ஸா பர்தசனி\n",
      "\n",
      "அக்ஸரா கெளதா\n",
      "\n",
      "அக்ஸரா ஹாசன்\n",
      "\n",
      "அக்ஸரா மேனன்\n",
      "\n",
      "அமலா\n",
      "\n",
      "அமலா பால்\n",
      "\n",
      "அம்பிகா\n",
      "\n",
      "அமீத்தா\n",
      "\n",
      "அமீஸா பாட்டீல்\n",
      "\n",
      "அமூல்யா\n",
      "\n",
      "அம்ரிதா அரோரா\n",
      "\n",
      "அம்ரிதா ப்ரகாஷ்\n",
      "\n",
      "அம்ரிதா ராவ்\n",
      "\n",
      "அம்ரிதா சிங்\n",
      "\n",
      "அம்ருதா காண்வில்கர்\n",
      "\n",
      "அமிரா டாஸ்டர்\n",
      "\n",
      "அனைக்கா சொட்டி\n",
      "\n",
      "அனன்யா\n",
      "\n",
      "அனஸ்வரா குமார்\n",
      "\n",
      "அனிதா ஹசநந்தனி\n",
      "\n",
      "அனிதா குஹா\n",
      "\n",
      "அனிதா ராஜ்\n",
      "\n",
      "அஞ்ஜலா ஷாவேரி\n",
      "\n",
      "அஞ்ஜலி \n",
      "\n",
      "அஞ்ஜலி தேவி\n",
      "\n",
      "அஞ்ஜலி சுதாகர்\n",
      "\n",
      "அஞ்ஜனா பெளமிக்\n",
      "\n",
      "அஞ்ஜனா ஸுகானி\n",
      "\n",
      "அஞ்ஜு மஹேந்த்ரா\n",
      "\n",
      "அன்கிதா லோகன்டே\n",
      "\n",
      "அன்கிதா ஸர்மா\n",
      "\n",
      "அன் அகஸ்டின்\n",
      "\n",
      "அன்ஸிபா ஹாசன்\n",
      "\n",
      "அன்தரா மலி\n",
      "\n",
      "அனு அகர்வால்\n",
      "\n",
      "அனு பிரபாகர்\n",
      "\n",
      "அனு சித்ரா\n",
      "\n",
      "அனுபமா பரமேஸ்வரன்\n",
      "\n",
      "அனுபமா வர்மா\n",
      "\n",
      "அனுராதா மேதா\n",
      "\n",
      "அனுஷா டன்டெகர்\n",
      "\n",
      "அனுஷ்கா சர்மா\n",
      "\n",
      "அனுஷ்கா ஸெட்டி\n",
      "\n",
      "அனுயா பக்வத்\n",
      "\n",
      "அபர்ஜிதா மேஹட்டி\n",
      "\n",
      "அபர்னா பாஜ்பாய்\n",
      "\n",
      "அபர்னா பாலமுரளி\n",
      "\n",
      "அபர்னா சென்\n",
      "\n",
      "அர்ச்சனா\n",
      "\n",
      "அர்ச்சனா ஜோஸ்\n",
      "\n",
      "அர்ச்சனா பூரன் சிங்\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(flist2[0], encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Extract the titles of all articles in the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 articles found\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "example_title ='<doc id=\"12\" url=\"https://ta.wikipedia.org/wiki?curid=12\" title=\"கட்டிடக்கலை\">'\n",
    "pattern = 'title=\"(.*?)\">'\n",
    "with open(flist2[0], encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "    titles=re.findall(pattern, text)\n",
    "    print(len(titles), 'articles found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8. Let us count all the articles we have downloaded across all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127381\n"
     ]
    }
   ],
   "source": [
    "pattern = 'title=\"(.*?)\">'\n",
    "def get_article_count(fname):\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "        titles=re.findall(pattern, text)\n",
    "        return len(titles)\n",
    "print(sum([get_article_count(f) for f in flist2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow, that is 127000+ articles! In Indian count 1.2 lakhs! Let us thank the great community of wiki writers that have done this wonder job of creating such a wide variety of valuable articles in Tamil. Let us each also try to add more articles on subjects we care about.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 9. Let us create a tiny subset of the tamil wiki for quick testing purpose. Let us create a set of 12000 articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let us rename the current text archive as tawiki_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv text tawiki_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tawiki_large/AE/AE_wiki_46.txt', 447)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flist3 = glob.glob('tawiki_large/*/*')\n",
    "flist3[0], len(flist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tawiki_large/AB/AB_wiki_17.txt', 'tawiki_large/AC/AC_wiki_73.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(flist3)\n",
    "flist3[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flist_small = flist3[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir tawiki_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in flist_small:\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        name=file.split('/')\n",
    "        newname=name[0].replace('large', 'small')+'/'+name[2]\n",
    "    with open(newname, \"w\") as text_file:\n",
    "        text_file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10. How many articles are on in this small sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11037\n"
     ]
    }
   ],
   "source": [
    "pattern = 'title=\"(.*?)\">'\n",
    "def get_article_count(fname):\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "        titles=re.findall(pattern, text)\n",
    "        return len(titles)\n",
    "    \n",
    "flist_small = glob.glob('tawiki_small/*')\n",
    "print(sum([get_article_count(f) for f in flist_small]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FastLM_Tamil.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
