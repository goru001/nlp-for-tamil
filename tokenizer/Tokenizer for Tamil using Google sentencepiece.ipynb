{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6axjnZg9kk4x"
   },
   "source": [
    "# Create a Tamil Tokenizer using Google SentencePiece\n",
    "\n",
    "Initial version: Ravi Annaswamy (April 6, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is tokenization?\n",
    "\n",
    "A 'word' in a language is usually a composite word. An example in English would be the word: 'unconditionally'.\n",
    "\n",
    "How many parts does it have?\n",
    "\n",
    "One could split it as: un-condition-al-ly\n",
    "\n",
    "We could say four:\n",
    "- prefix: un\n",
    "- root is: condition\n",
    "- first suffix: al\n",
    "- second suffix is: ly\n",
    "\n",
    "\n",
    "### Why Tokenize?\n",
    "\n",
    "Why should we split? In order to make sense of its meaning. The prefixes and suffixes modify the core meaning to put it to some specific use. Some prefixes are syntactic, some semantic. \n",
    "\n",
    "This is even more composite in Indic languages where EVERY verb and noun is an inflection form. In English, verbs take on tense and sometimes person. In almost all Indian languages, verbs take on tense, person, count, gender.\n",
    "In Tamil additionally, nouns take on count, proposition too and verb form takes on the conjunction.\n",
    "\n",
    "Take an example tamil word: அமைந்துள்ளதால்\n",
    "This means: \"because it is situated\" \n",
    " அமைந்துள்ளதால் = அமை ந்த ுள்ள து ஆல்\n",
    " அமை (situate) ந்த (d) ுள்ள (is) து (it) ஆல் (because)\n",
    " \n",
    " To give another concrete illustration, a word such as, 'வந்திருந்தானா?' really is a phrase (Had he come?) splits to 'வ' 'ந்த' 'இரு' 'ந்த' 'ஆன்' 'ஆ', compactly encoding almost one syllable each for the verb, past tense, participle, past tense of the participle, gender and person and number, query marker respectively!\n",
    "\n",
    "Seeing a word as parts is so essential for understanding. These parts are called as tokens and splitting a word to its pieces is called tokenization.\n",
    "\n",
    "A translation engine between Tamil and English can now correlate word piece meanings and learn the root is verb!\n",
    "\n",
    "### How to create a tokenizer?\n",
    "\n",
    "In traditional NLP, tokenizers and lemmatizers were initially hand constructed. Then they were machine learned from large corpus of data that has been tagged by humans. With modern machine learning one can construct reasonable tokenizers just using information theory and optimization principles. \n",
    "\n",
    "Google sentencepiece software (similar to BPE) learns a vocabulary that optimally can construct any word in a corpus.\n",
    "\n",
    "From raw unsegmented text, if you give it a vocabulary size, say 20000 words, it figures out 20000 word-parts that will give optimal coverage of entire text.\n",
    "\n",
    " That is 20000 words are given token ids, rest of the words are composed using this.\n",
    "\n",
    "\n",
    " Sentencepiece (and similar technique called Byte Pair Encoding (BPE) truly serve well as an automatically constructed lemmatizer and tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 1. Install Google Sentencepiece software. This automatically figures out word-parts (roots, suffixes, prefixes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Get list of files from Tamil Wikipedia (or any corpus you would like) to serve as input for sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to test this quickly, you can point to tawiki_small instead of large, to get a smaller dataset to train the sentence piece model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TA_WIKI_PATH = '../corpus/tawiki/tawiki_large/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "flist = glob.glob(TA_WIKI_PATH+'*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "exm558JLkpL6",
    "outputId": "9b11509c-b8f4-4e30-e714-c49ee12de704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../01_corpus/tawiki/tawiki_large/AA/AA_wiki_65.txt\n",
      "<doc id=\"45501\" url=\"https://ta.wikipedia.org/wiki?curid=45501\" title=\"அணுக்கரு விசை\">\n",
      "அணுக்கரு விசை\n",
      "\n",
      "அணுக்கரு விசை (\"Nuclear force\") எனப்படுவது இரண்டு அல்லது அதற்கு மேற்பட்ட அணுக்கருனிகளுக்கிடையே ஏற்படும் விசை ஆகும். அணுவின் உட்கருவில் உள்ள நேர் மின்னூட்டம் கொண்ட புரோட்டான்களுக்கிடையில் உள்ள மின்னூட்ட விலக்கல் விசையை விட அதிகமான அளவில் இந்த அணுக்கரு விசை செயற்பட்டு புரோட்டான்களையும் மின்னூட்டம் அற்ற நியூட்ரான்களையும் பிணைக்க வைக்கின்றது.\n",
      "\n",
      "1932 ஆம் ஆண்டில் ஜேம்ஸ் சாட்விக் என்பவரால் நியூட்டாரன் கண்டுபிடிக்கப்பட்டதில் இருந்து அணுக்கரு விசை அணுக்கருவியலில் முக்கியத்துவம் பெற்றது.\n",
      "\n",
      "1935 ஆம் ஆண்டில் ஹிடெக்கி யுக்காவா என்பவர் அணுக்கரு விசை பற்றி முதன் முதலாக எடுத்துக் கூறினார். அவரது கூற்றுப்படி, இரு அணுக்கருனிகளுக்கிடையே தொடர்ந்தாற் போல் போசோன்கள் (மீசோன்கள்) என்ற துகள் பரிமாற்றம் நடைபெறுகிறது. எனினும், மீசோன் தத்துவம் தற்போது இவ்விசையின் அடிப்படைக் கொள்கை என ஏற்றுக் கொள்ளப்படாவிட்டாலும், இந்த மீசோன் துகள் பரிமாற்றக் கொள்கை \"நியூட்ரான்-நியூட்ரான்\" அழுத்தத்தின் நேரடியான சோதனைகளுக்கு இக்கொள்க\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "fname=random.choice(flist)\n",
    "print(fname)\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Make this filelist into a long comma-separated string to feed sentencepiece trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../01_corpus/tawiki/tawiki_large/AE/AE_wiki_46.txt,../01_corpus/tawiki/tawiki_large/AE/AE_wiki_34.txt,../01_corpus/tawiki/tawiki_large/AE/AE_wiki_04.txt,../01_corpus/tawiki/tawiki_large/AE/AE_wiki_42.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flist2=','.join(flist)\n",
    "flist2[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Call Sentencepiece Train function, asking it to create a spm model with 8000 word parts.\n",
    "\n",
    "8000 may seem very small, but I found this optimal for language modeling, translation software. \n",
    "I suspect that this is because almost all tamil verbs are two syllables, and tense, number, gender, person are all \n",
    "encoded in one syllable each!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hl_6DcWhlWIY",
    "outputId": "378d8cbf-7f79-49d4-cc89-768028bb1d3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.Train(f'--input={flist2} --model_prefix=tamil_spm_8k --vocab_size=8000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Now Load this model into a processor object and ask it to tokenize pieces of text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lCcwkAq3l6bw"
   },
   "outputs": [],
   "source": [
    "spt = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FcLW7YCxl__P",
    "outputId": "43d26a79-e89f-473d-89dc-25225c3f9774"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spt.Load(\"tamil_spm_8k.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you can pass any text and it will return tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁அமைந்துள்ள', 'தால்']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spt.EncodeAsPieces('அமைந்துள்ளதால்')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17307
    },
    "colab_type": "code",
    "id": "2jfeEM-EmCGP",
    "outputId": "d3408dd8-bd23-43f9-b1ff-baea68203435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁< doc ▁id =\" 45 50 1 \" ▁ url =\" https :// ta . wikipedia . org / wiki ? curi d = 45 50 1 \" ▁title =\" அ ணு க்கரு ▁விசை \"> ▁அணுக்கரு ▁விசை ▁அணுக்கரு ▁விசை ▁(\" N uc le ar ▁for ce \") ▁எனப்படுவது ▁இரண்டு ▁அல்லது ▁அதற்கு ▁மேற்பட்ட ▁அணுக்கரு னி களுக்கிடையே ▁ஏற்படும் ▁விசை ▁ஆகும் . ▁அணு வின் ▁உட் கரு வில் ▁உள்ள ▁நேர் ▁மின்ன ூட்ட ம் ▁கொண்ட ▁புரோ ட்டான் களுக்க ிட ையில் ▁உள்ள ▁மின்ன ூட்ட ▁விலக்க ல் ▁விசை யை ▁விட ▁அதிகமான ▁அளவில் ▁இந்த ▁அணுக்கரு ▁விசை ▁செயற் பட்டு ▁புரோ ட்டான் களையும் ▁மின்ன ூட்ட ம் ▁அ ற்ற ▁நியூ ட்ரா ன் களையும் ▁பிணை க்க ▁வைக்க ின்றது . ▁19 32 ▁ஆம் ▁ஆண்டில் ▁ஜேம்ஸ் ▁சா ட் வி க் ▁என்பவரால் ▁நியூ ட்டா ரன் ▁கண்டுபிடிக்க ப்பட்ட தில் ▁இருந்து ▁அணுக்கரு ▁விசை ▁அணுக்கரு வியலில் ▁முக்கியத்துவம் ▁பெற்றது . ▁1935 ▁ஆம் ▁ஆண்டில் ▁ஹி டெ க்கி ▁யு க்கா வா ▁என்பவர் ▁அணுக்கரு ▁விசை ▁பற்றி ▁முதன் ▁முதலாக ▁எடுத்துக் ▁கூறினார் . ▁அவரது ▁கூற்றுப்படி , ▁இரு ▁அணுக்கரு னி களுக்கிடையே ▁தொடர்ந்த ா ற் ▁போல் ▁போ சோ ன் கள் ▁( மீ சோ ன் கள் ) ▁என்ற ▁துகள் ▁பரிமாற்ற ம் ▁நடைபெறுகிறது . ▁எனினும் , ▁மீ சோ ன் ▁தத்துவ ம் ▁தற்போது ▁இவ்வி சை யின் ▁அடிப்படை க் ▁கொள்கை ▁என ▁ஏற்றுக் ▁கொள்ள ப்ப டா விட்ட ாலும் , ▁இந்த ▁மீ சோ ன் ▁துகள் ▁பரிமாற்ற க் ▁கொள்கை ▁\" நியூ ட்ரா ன் - நியூ ட்ரா ன் \" ▁அழுத்த த்தின் ▁நேரடி யான ▁சோதனை களுக்கு ▁இக் கொள் கை ▁இப்போது ம் ▁பயன்படுத்த ப்படுகிறது . ▁</ doc > ▁< doc ▁id =\" 45 50 2 \" ▁ url =\" https :// ta . wikipedia . org / wiki ? curi d = 45 50 2 \" ▁title =\" பெர் மி ▁( அ ல கு ) \"> ▁பெர் மி ▁( அ ல கு ) ▁பெர் மி ▁(\" f er mi \") ▁என்பது ▁நீள த்துக்கான ▁S I ▁அலகு த் ▁தீ ட்ட ம ல்லாத ▁ஒரு ▁அலக ாகும் . ▁ஆனாலும் , ▁இது ▁S I - அ லக ான ▁பெ ம் டோ மீ\n"
     ]
    }
   ],
   "source": [
    "tok_text = spt.EncodeAsPieces(text[:1250])\n",
    "print(' '.join(tok_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please how it has split words - you get the picture!\n",
    "'▁நியூ ட்ரா ன் களையும்',\n",
    "'▁பாதுகாப்ப',\n",
    " 'வர்',\n",
    " 'களை'\n",
    "'▁அணி',\n",
    " 'ந்த',\n",
    " 'வர்',\n",
    " 'களாகவும்',\n",
    " '▁இருப்ப',\n",
    " 'வர்',\n",
    " 'களாகவும்',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "colab_type": "code",
    "id": "tks0pUK7mawy",
    "outputId": "26151ddd-869c-463e-ba36-7f589b487000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\t0\r\n",
      "<s>\t0\r\n",
      "</s>\t0\r\n",
      ".\t-3.04693\r\n",
      ",\t-3.73844\r\n",
      "▁\t-4.33228\r\n",
      "\"\t-4.65909\r\n",
      "ம்\t-4.69299\r\n",
      "=\"\t-4.75747\r\n",
      "க்\t-4.94869\r\n",
      "ப்\t-4.99399\r\n",
      "doc\t-5.07301\r\n",
      "கள்\t-5.07628\r\n",
      "்\t-5.3333\r\n",
      ")\t-5.38481\r\n",
      "▁(\t-5.42036\r\n",
      "த்\t-5.42682\r\n",
      "▁ஒரு\t-5.49748\r\n",
      "▁மற்றும்\t-5.50963\r\n",
      "d\t-5.58061\r\n",
      "/\t-5.64493\r\n",
      ">\t-5.69503\r\n",
      "ன்\t-5.70746\r\n",
      "ta\t-5.72214\r\n",
      "ு\t-5.73666\r\n",
      "?\t-5.73758\r\n",
      "▁<\t-5.73955\r\n",
      "://\t-5.75524\r\n",
      "org\t-5.75924\r\n",
      "https\t-5.76287\r\n",
      "▁</\t-5.76389\r\n",
      "wikipedia\t-5.76452\r\n",
      "wiki\t-5.76453\r\n",
      "url\t-5.7652\r\n",
      "curi\t-5.766\r\n",
      "▁title\t-5.76649\r\n",
      "▁id\t-5.76664\r\n",
      "\">\t-5.77756\r\n",
      "ல்\t-5.78961\r\n",
      "ும்\t-5.8505\r\n",
      "ர்\t-5.85397\r\n",
      "து\t-5.85483\r\n",
      "▁\"\t-5.89759\r\n",
      "▁இந்த\t-5.93694\r\n",
      "ய\t-5.97812\r\n",
      "யில்\t-6.02099\r\n",
      "ச்\t-6.02583\r\n",
      "ஸ்\t-6.03458\r\n",
      "த்தில்\t-6.07806\r\n",
      "=\t-6.10535\r\n"
     ]
    }
   ],
   "source": [
    "!head -50 tamil_spm_8k.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just need the following two files (model and vocab) to use the learned tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamil_spm_8k.model  Tokenizer for Tamil using Google sentencepiece.ipynb\r\n",
      "tamil_spm_8k.vocab\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Let us also train a 16000 word tamil word segmentation model for experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.Train(f'--input={flist2} --model_prefix=tamil_spm_16k --vocab_size=16000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spt2 = spm.SentencePieceProcessor()\n",
    "spt2.Load(\"tamil_spm_16k.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁அமைந்துள்ள', 'தால்']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spt2.EncodeAsPieces('அமைந்துள்ளதால்')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u17qW1TnkK7-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁< doc ▁id =\" 45 501\" ▁ url =\" https :// ta . wikipedia . org / wiki ? curi d =45 501\" ▁title =\" அ ணு க்கரு ▁விசை \"> ▁அணுக்கரு ▁விசை ▁அணுக்கரு ▁விசை ▁(\" N uc le ar ▁for ce \") ▁எனப்படுவது ▁இரண்டு ▁அல்லது ▁அதற்கு ▁மேற்பட்ட ▁அணுக்கரு னி களுக்கிடையே ▁ஏற்படும் ▁விசை ▁ஆகும் . ▁அணு வின் ▁உட்கரு வில் ▁உள்ள ▁நேர் ▁மின்னூட்ட ம் ▁கொண்ட ▁புரோட்டான் களுக்கிடையில் ▁உள்ள ▁மின்னூட்ட ▁விலக்க ல் ▁விசை யை ▁விட ▁அதிகமான ▁அளவில் ▁இந்த ▁அணுக்கரு ▁விசை ▁செயற்பட்டு ▁புரோட்டான் களையும் ▁மின்னூட்ட ம் ▁அற்ற ▁நியூ ட்ரான் களையும் ▁பிணை க்க ▁வைக்க ின்றது . ▁1932 ▁ஆம் ▁ஆண்டில் ▁ஜேம்ஸ் ▁சா ட் விக் ▁என்பவரால் ▁நியூ ட்டா ரன் ▁கண்டுபிடிக்க ப்பட்ட தில் ▁இருந்து ▁அணுக்கரு ▁விசை ▁அணுக்கரு வியலில் ▁முக்கியத்துவம் ▁பெற்றது . ▁1935 ▁ஆம் ▁ஆண்டில் ▁ஹி டெ க்கி ▁யு க்கா வா ▁என்பவர் ▁அணுக்கரு ▁விசை ▁பற்றி ▁முதன் ▁முதலாக ▁எடுத்துக் ▁கூறினார் . ▁அவரது ▁கூற்றுப்படி , ▁இரு ▁அணுக்கரு னி களுக்கிடையே ▁தொடர்ந்த ாற் ▁போல் ▁போ சோ ன் கள் ▁( மீ சோ ன் கள் ) ▁என்ற ▁துகள் ▁பரிமாற்றம் ▁நடைபெறுகிறது . ▁எனினும் , ▁மீ சோ ன் ▁தத்துவம் ▁தற்போது ▁இவ்வி சை யின் ▁அடிப்படை க் ▁கொள்கை ▁என ▁ஏற்றுக் ▁கொள்ள ப்பட ாவிட்டாலும் , ▁இந்த ▁மீ சோ ன் ▁துகள் ▁பரிமாற்ற க் ▁கொள்கை ▁\" நியூ ட்ரான் - நியூ ட்ரான் \" ▁அழுத்த த்தின் ▁நேரடி யான ▁சோதனை களுக்கு ▁இ க்கொள்கை ▁இப்போது ம் ▁பயன்படுத்த ப்படுகிறது . ▁</ doc > ▁< doc ▁id =\" 45 502\" ▁ url =\" https :// ta . wikipedia . org / wiki ? curi d =45 502\" ▁title =\" பெர் மி ▁( அ லகு ) \"> ▁பெர் மி ▁( அ லகு ) ▁பெர் மி ▁(\" fer mi \") ▁என்பது ▁நீள த்துக்கான ▁ SI ▁அலகு த் ▁தீட்ட ம ல்லாத ▁ஒரு ▁அலக ாகும் . ▁ஆனாலும் , ▁இது ▁ SI - அ லக ான ▁பெ ம் டோ மீ\n"
     ]
    }
   ],
   "source": [
    "tok_text_16k = spt2.EncodeAsPieces(text[:1250])\n",
    "print(' '.join(tok_text_16k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Visually compare tokenization performance of 16k model and 8k model to see which seems natural for Tamil use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "வித்யாராஜாக்கள் பெரும்பாலும் உக்கிர மூர்த்திகளைப் போல் பல முகங்கள், கரங்கள் முதலியவற்றோடு சித்திரக்கப்படுகின்றனர். கையில் ஆயுதங்கள் ஏந்தியவாறும் சில சிமயங்களில் கபாலம் மற்றும் மிருக தோல்களை அணிந்தவர்களாகவும் தீப்பிழம்புகள் சூழ இருப்பவர்களாகவும் காட்சியளிக்கின்றனர்.\n"
     ]
    }
   ],
   "source": [
    "# create normal test\n",
    "test_text = '▁வித்யா ராஜா க்கள் ▁பெரும்பாலும் ▁உ க்கிர ▁மூர்த்தி களைப் ▁போல் ▁பல ▁முக ங்கள் , ▁கர ங்கள் ▁முதலியவற்ற ோடு ▁சித்திர க்க ப்படுகின்றனர் . ▁கையில் ▁ஆயுதங்கள் ▁ஏ ந்திய வாறு ம் ▁சில ▁சி ம ய ங்களில் ▁க பால ம் ▁மற்றும் ▁மிரு க ▁தோல் களை ▁அணிந்த வர்களாகவும் ▁தீ ப்பி ழ ம்பு கள் ▁சூழ ▁இருப்பவர்கள ாகவும் ▁காட்சியளிக்க ின்றனர் .'\n",
    "test_text=test_text.replace(' ','').replace('▁',' ').strip()\n",
    "print(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8k tokenization: ▁வித்யா ராஜா க்கள் ▁பெரும்பாலும் ▁உ க்கிர ▁ மூர்த்தி களைப் ▁போல் ▁பல ▁முக ங்கள் , ▁கர ங்கள் ▁முதலிய வற்ற ோடு ▁சித்திர க்க ப்படுகின்றனர் . ▁கை யில் ▁ஆயுத ங்கள் ▁ஏ ந்திய வாறு ம் ▁சில ▁சி மய ங்களில் ▁க பால ம் ▁மற்றும் ▁மிருக ▁தோல் களை ▁அணி ந்த வர் களாகவும் ▁தீ ப்பி ழ ம்பு கள் ▁சூழ ▁இருப்ப வர் களாகவும் ▁காட்சி யளிக்க ின்றனர் .\n",
      "\n",
      "16k tokenization: ▁வித்யா ராஜா க்கள் ▁பெரும்பாலும் ▁உக்கிர ▁மூர்த்தி களைப் ▁போல் ▁பல ▁முக ங்கள் , ▁கர ங்கள் ▁முதலியவற்ற ோடு ▁சித்திர க்க ப்படுகின்றனர் . ▁கையில் ▁ஆயுதங்கள் ▁ஏ ந்திய வாறு ம் ▁சில ▁சி மய ங்களில் ▁க பால ம் ▁மற்றும் ▁மிருக ▁தோல் களை ▁அணிந்த வர்களாகவும் ▁தீ ப்பி ழ ம்பு கள் ▁சூழ ▁இருப்பவர் களாகவும் ▁காட்சி யளிக்க ின்றனர் .\n"
     ]
    }
   ],
   "source": [
    "tok_text_8k = spt.EncodeAsPieces(test_text)\n",
    "print('8k tokenization:',' '.join(tok_text_8k))\n",
    "print()\n",
    "tok_text_16k = spt2.EncodeAsPieces(test_text[:1250])\n",
    "print('16k tokenization:',' '.join(tok_text_16k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice? Do you notice the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what I see: .\n",
    "    ▁ஆயுத ங்கள் -> ▁ஆயுதங்கள்  (weapons)\n",
    "    ▁அணி ந்த வர் களாகவும் -> ▁அணிந்த வர்களாகவும் [(they) as adorning (animal skin)].\n",
    "    \n",
    "    \n",
    "The 8k resource-limited model correctly splits 'weapons' into weapon+s, \n",
    "\n",
    "the resource-rich 16K model allocation a new word for the plural form.\n",
    "\n",
    "Same is the case for second word: 8k splits it to 4 parts \n",
    " - core verb, \n",
    " - past tense marker, \n",
    " - third person marker, and \n",
    " - number marker+proposition.\n",
    " \n",
    " The 16k model splits into 2 parts.\n",
    " I like the 8k better because it has teased out the latent variables better.\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Some themes in history and philosophy (without the politics)\n",
    "\n",
    "Most Indian languages have solid grammatical foundations in morphology and phonetics.\n",
    "\n",
    "While we normally think of grammars as specifying word classes (such as nouns, verbs and their modifiers adjectives, adverbs and degree markers etc) and sentence making rules, Indian grammars specify rules for word making and even sound making so that a language can be precisely spoken across vast land masses if needed without dialect distortions.\n",
    "\n",
    "So the forms of Tamil and Sanskrit and their children are in some sense *designed* languages (or at the least *explicitly refined* languages).\n",
    "\n",
    "Tholkappiyam (தொல்காப்பியம் http://www.tholkappiyam.org/) and Nannool (நன்னூல்) are ancient Tamil grammars explicitly specify how various modifiers adorn verbs or nouns in addition to specifying almost every aspect of word making, phrase making, sentence making and meaning making and language evolution. \n",
    "\n",
    "Panini's Astadhyayi (http://ashtadhyayi.com/sutraani/) likewise is an extremely concise grammar for Sanskrit and was one of the inspiration behind modern programming language design via Sassuire, BNF, Chomsky path. \n",
    "\n",
    "Panini takes grammar description to almost information theoretic obsession of a codebook, a minimal-code codebook - to the degree the text of his spec is not meant to be 'human' readable but like a book of mathematical formula, only describes latent entities and interactions that turn in to language later, through progressive refinement. In fact he moved beyond writing a 'descriptive' grammar to a 'generative grammar' which led to the CFG like spec. Sentence formulae that govern creation of ANY sentence.\n",
    "\n",
    "As a result of such rich design in language and the ensuing regularity of the governed languages, even simple information theoretic analysis of raw text, could quickly reveals the core grammar of such languages. Latin, German and hundreds of other languages also have such properties and my guess is that using a small 'core' wordpart parameter will make their language models simpler too.\n",
    "\n",
    "It is a matter of pride for all of us humans, that Indians, Chinese and Middle Eastern world had made such breakthroughs in systematization of written and spoken languages over 5000 years ago and had well specified grammars 2000 years ago. And also a matter of pride that Europeans and western world have built upon those breakthroughs to tie them to computation and programming language and now we are able to make one (Computation) help understand the other (Language). Final matter of joy that internet, globalization, computing, storage breakthroughs in the last two decades have established such an amount of knowledge sharing and collaborative tools that now every one across the world can help each other as members of one family working towards common goal of improving every one's life.\n",
    "\n",
    "Later we will build language models for one language from each of major groups: Indian, European, Chinese, Russian, African families and try to do our own quick study of common roots and language geneologies. Now you know the value of human-created and curated treasure that is Wikipedia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Hope \n",
    "\n",
    "Past: Human grammarians created language models by studying language usage and theorizing on that. These grammars were then used to teach people how to communicate with each other quickly and without confusion.\n",
    "\n",
    "Present: In the last 100 years, these ideas combined with computation and information theory, led to language models for communicating efficiently and effectively to a computer.\n",
    "\n",
    "Present-present: We are at a point where machines are able to create language models from natural language using statistical techniques, with less and less human guidance, tedium and bias.\n",
    "\n",
    "Future: We hope that computer generated grammars will be far more effective in parsing/understanding natural text because they can gain experience on vast vast amounts of data with less choice of bias and prejudice over long times. They could help humans create new era of multi-lingual and cross lingual unified grammars and our ability to communicate to each other across globe will get a huge lift, soon. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to topic, we have 2 tokenizers to choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamil_spm_16k.model  tamil_spm_8k.vocab\r\n",
      "tamil_spm_16k.vocab  Tokenizer for Tamil using Google sentencepiece.ipynb\r\n",
      "tamil_spm_8k.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step we will build a language model (which is a collection of automatically learning syntactic, semantic and pragmatic rules that help predict what word will come next in a sequence of a real-world sentence).\n",
    "\n",
    "We will learn this using a powerful library called fastai from a generous genius, *Jeremy Howard* (https://en.wikipedia.org/wiki/Jeremy_Howard_(entrepreneur)) \n",
    "\n",
    "We will learn it from large wikipedia article collection which has wide coverage of articles, topics, sentence formats to inform our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FastLM_Tamil.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
